import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import itertools
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC
#pip install graphviz
import graphviz
from sklearn.tree import export_graphviz

corn_data = pd.read_csv ('vcf23_1.csv')

categories = {"A": 1, "B": 2, "AB": 3, "A_TESTER": 4, "B_TESTER": 5}
corn_data['Class']= corn_data['Class'].map(categories)


X = corn_data.drop(['CMLsid','Class'], axis=1) 
y = corn_data['Class']
X = pd.DataFrame(X).fillna(0)
y = pd.DataFrame(y).fillna(2.0)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""
stratify = y - точность менялась только в зависимости от значений random_state
"""

clf = RandomForestClassifier(oob_score = True)
parameters = {'n_estimators': [70, 90, 100, 110], 'max_depth': [2, 5, 7], 
              'min_samples_leaf': [2, 3, 5], 'max_features': [8, 13, 20, 30, 40, 50, 75, 100, 'sqrt']}
grid_search_cv_clf = GridSearchCV (clf, parameters, cv = 5)
grid_search_cv_clf.fit(X_train, y_train)
grid_search_cv_clf.best_params_

'''
варианты введения oob_score - напрямую gridsearch через make_scorer провести не удалось, но можно попробовать цикл
'''
scores_data = pd.DataFrame
max_depth_values = range(1, 10) #в качестве варьируемого может быть выбран иной параметр
for max_depth in max_depth_values:
    clf = RandomForestClassifier(criterion='gini', max_depth=max_depth, n_estimators = 110, min_samples_split= 5, min_samples_leaf = 2, oob_score = True)
    clf.fit(X_train, y_train)
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    oob_score = clf.oob_score
    temp_score_data = pd.DataFrame({'max_depth': [max_depth], 'train_score': [train_score], 'test_score': [test_score], 'oob_score': [oob_score]})
    scores_data = scores_data.append(temp_score_data)


best_clf = grid_search_cv_clf.best_estimator_ 
best_clf 
best_clf.score(X_train, y_train) #проверка переобученности
best_clf.score(X_test, y_test) 
best_clf.oob_score_
y_pred = best_clf.predict(X_test)

print(accuracy_score(y_test, y_pred))  
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

'''
Дополнение за 19.10 - черновой код для реализации классификатора random forest + snp:
Причина: в рамках перебора вариантов с непосредственно RF проблема переобучаемости пока не преодолена
Алгоритм: работа с RF - выявление фич - работа SVM с выбранными RF фичами

Ближайшие изменения будут связаны с:
Кросс-валидация: использование стратифицированной кросс-валидации; переход на out-of-bag verification
RF-SVM: работа над более "экологичным" внедрением в код метода

'''
n_features = 310
snp = corn_data.columns.values
snp_rank = dict(zip(snp, best_clf.feature_importances_))
feature_rank = dict(itertools.islice(snp_rank.items(), n_features))
top_snp = list(feature_rank.keys())
top_snp = np.asarray(top_snp) 
X_train = X_train[top_snp].values
X_test = X_test[top_snp].values

clf = SVC(kernel='rbf', decision_function_shape='ovo')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(accuracy_score(y_test, y_pred))  
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""
дальнейший код - иные методы поиска/проверки параметров
и для GridSearch и для RandomizedSearch совершаются запуски при для отдельных значений параметров,
поскольку не исключена возможность неправильного применения кросс-валидации
"""

rf_classifier = RandomForestClassifier(
                      oob_score=True,
                      random_state=42)
                      
max_features = [8, 13, 20, 30, 40, 50, 75, 100, 'log2']
n_estimators = range(80, 150)
max_depth = [3, 5, 7]
min_samples_split = range(2, 5)
min_samples_leaf = range(2, 4)
   
random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'max_features': max_features}
               
rf_random = RandomizedSearchCV(
                estimator = rf,
                param_distributions = random_grid,
                n_iter = 50, cv = 5,
                verbose=1, random_state=seed)
               
rf_random.fit(X_train, y_train)
rf_random.best_params_
best_model = rf_random.best_estimator_
best_model.score(X_train, y_train)
best_model.score(X_test, y_test)
best_model.oob_score
y_pred = best_model.predict(X_test)

print(accuracy_score(y_test, y_pred))  
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))


clf_rf = RandomForestClassifier(max_features='sqrt', criterion='gini', n_estimators = 110, 
                                min_samples_leaf = 2, max_depth = 5, oob_score = True, random_state = 42)
clf_rf.fit(X_train, y_train)
clf_rf.score(X_test, y_test)
clf_rf.oob_score
y_pred = clf.predict(X_test)

print(accuracy_score(y_test, y_pred))  
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))




# код для roc-кривой

n_classes = len(np.unique(y_test))
y_test = label_binarize(y_test, classes=np.arange(n_classes))
y_pred = label_binarize(y_pred, classes=np.arange(n_classes))

fpr = dict()
tpr = dict()

y_test=y_test.reshape((190,1))
y_pred=y_pred.reshape((190,1))
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_train[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

plt.figure(dpi=600, figsize=(10,5))
lw = 2

plt.plot(fpr["micro"], tpr["micro"],
    label="micro-average ROC curve (area = {0:0.2f})".format(roc_auc["micro"]),
    color="deeppink", linestyle=":", linewidth=4,)

plt.plot(fpr["macro"], tpr["macro"],
    label="macro-average ROC curve (area = {0:0.2f})".format(roc_auc["macro"]),
    color="navy", linestyle=":", linewidth=4,)

colors = cycle(["aqua", "darkorange", "darkgreen", "yellow", "blue"])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
        label="ROC curve of class {0} (area = {1:0.2f})".format(i, roc_auc[i]),)

plt.plot([0, 1], [0, 1], "k--", lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) curve")
plt.legend()

#visualize decision tree

vizclf = export_graphviz(rf.estimators_[110], X, y,
               target_name="heterotic_group",
               feature_names=corn_data.feature_names,
               class_names=list(heterotic_group.names),
               filled=True, impurity=True, 
                           rounded=True)
#list index out of range

graph = graphviz.Source(dot_data, format='png')
graph
graph.render('figure_name')
